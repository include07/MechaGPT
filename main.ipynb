{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmJyDmws6w6j"
      },
      "source": [
        "# MechaGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0U0FHEJj1Fa"
      },
      "source": [
        "## Setup and dependencies installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j6qDqU_XqFr",
        "outputId": "65e9fbb6-1a1d-41a5-a1df-f9d25ae142f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoOpSaZKyjDE"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install --quiet --upgrade langchain langchain-community langchain-chroma layoutparser opencv-python torchvision pdf2img\n",
        "python3 -m pip install -U 'git+https://github.com/facebookresearch/detectron2.git@ff53992b1985b63bd3262b5a36167098e3dada02'\n",
        "pip install \"layoutparser[ocr]\"\n",
        "git clone https://github.com/Layout-Parser/layout-parser.git\n",
        "cd layout-parser/\n",
        "sudo apt-get install poppler-utils\n",
        "sudo apt-get install tesseract-ocr-eng\n",
        "pip install -U 'git+https://github.com/nikhilweee/iopath'\n",
        "pip install --upgrade Pillow\n",
        "pip install Pillow==9.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywji-F1nKEnL"
      },
      "source": [
        "## Layoutparser testing with Meta's detectron2 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "smr3QYKjf5xg"
      },
      "outputs": [],
      "source": [
        "import layoutparser as lp\n",
        "import cv2\n",
        "from PIL import Image as im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i14jDDSEybhz"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(\"/content/drive/MyDrive/Simulation_IA_24_25_JALALEDDIN_EL_FIRQI/Projet d'Intiation - MechaGPT/capture2.png\")\n",
        "image = image[..., ::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqI_WddSzqnI",
        "outputId": "ebd88880-546f-4962-9742-c4e06c815900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
          ]
        }
      ],
      "source": [
        "# Load the deep layout model from the layoutparser API\n",
        "# For all the supported model, please check the Model\n",
        "# Zoo Page: https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html\n",
        "\n",
        "model3 = lp.Detectron2LayoutModel('lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',\n",
        "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n",
        "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bNgW9DVSAceU"
      },
      "outputs": [],
      "source": [
        "# Detect the layout of the input image\n",
        "layout3 = model3.detect(image)\n",
        "\n",
        "sorted_layout3 = sorted(layout3, key=lambda x: x.coordinates[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1rZektpX90aroGE3OcnAhmHp2RIiJhkSk",
      "authorship_tag": "ABX9TyPrngLB8GU0OY87d9IjtdqJ"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}